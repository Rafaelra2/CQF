{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project - Ensemble ML for SPY Price Trend Prediction\n",
    "\n",
    "## Tuning using optuna \n",
    "\n",
    "### Random Forest, KNN, and SVC \n",
    "\n",
    "#_________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-19 18:02:00.297804: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-01-19 18:02:00.297869: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "## Data Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "## Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.io as pio\n",
    "pio.kaleido.scope.mathjax = None \n",
    "\n",
    "## Model Building and Evaluating\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestClassifier# Base Model 1\n",
    "from sklearn.neighbors import KNeighborsClassifier # Base Model 2\n",
    "from sklearn.svm import SVC # Base Model 3\n",
    "from xgboost import XGBClassifier # Meta Model  \n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "\n",
    "\n",
    "## Improve Opmization\n",
    "import optuna # Allows you to define an objective function, which you want to optimize\n",
    "from optuna.visualization import plot_slice, plot_optimization_history\n",
    "from functools import partial # Provides higher-order functions and operations on callable objects\n",
    "\n",
    "\n",
    "\n",
    "## Import Functions and Models\n",
    "from functions import *\n",
    "from build_models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load dataset\n",
    "df = pd.read_csv('../../Data/df_final.csv', index_col=0)\n",
    "\n",
    "## Specify Features and Target\n",
    "X, y = df.iloc[:,:-1].values, df.iloc[:,-1].values\n",
    "\n",
    "## Set TimeSplit \n",
    "TimeSplit = 3\n",
    "\n",
    "## Number of trials\n",
    "n_trials = 100\n",
    "\n",
    "## Set Random State\n",
    "rs = 55\n",
    "\n",
    "## Set a seed\n",
    "set_seeds(seed=rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Objective Functions:  \n",
    "Evaluation Metric: Max f-score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  1) Random Forest\n",
    "def optimize_RF(trail, x, y):\n",
    "\n",
    "    ## Specify Hyperparameters range\n",
    "    n_estimators = trail.suggest_int(\"n_estimators\", 100, 1500)\n",
    "\n",
    "    max_depth = trail.suggest_int(\"max_depth\", 3, 15)\n",
    "\n",
    "    max_features = trail.suggest_float(\"max_features\", 0.01, 1.0)\n",
    "\n",
    "    ## Build Model\n",
    "    model = RandomForestClassifier( n_jobs=-1, random_state=rs, \n",
    "                                    class_weight= cwts(y), # Balance Class\n",
    "                                    n_estimators=n_estimators, # Set the number of trees (estimators).  \n",
    "                                    max_depth=max_depth, # Controls the complexity of each tree. \n",
    "                                    max_features = max_features, \n",
    "                                    min_samples_leaf=1,  # Minimum number of samples in a leaf node\n",
    "                                    min_samples_split=2, # Min samples required to split an internal node in a decision tree\n",
    "                                    criterion='gini'  # Use Gini impurity as the split criterion. \n",
    "                                    )\n",
    "\n",
    "    ## Set TS-CV\n",
    "    tscv = TimeSeriesSplit(n_splits=TimeSplit, gap=1)\n",
    "\n",
    "    ## Create Variable\n",
    "    fscore = []\n",
    "\n",
    "    ## Run for the different training and testing data from TS-CV splits \n",
    "    for idx in tscv.split(x):\n",
    "        train_idx, test_idx = idx[0], idx[1]\n",
    "\n",
    "        xtrain = x[train_idx]\n",
    "        ytrain = y[train_idx]\n",
    "\n",
    "        xtest = x[test_idx]\n",
    "        ytest = y[test_idx]\n",
    "\n",
    "        model.fit(xtrain, ytrain)\n",
    "        preds = model.predict(xtest)\n",
    "\n",
    "        cv_f1 = f1_score(ytest, preds, average='weighted')\n",
    "        fscore.append(cv_f1)\n",
    "\n",
    "    return np.mean(fscore)\n",
    "\n",
    "\n",
    "#______________________________________________________________________________\n",
    "\n",
    "###  2) KNN\n",
    "\n",
    "def optimize_KNN(trail, x, y):\n",
    "\n",
    "    ## Specify Hyperparameters range\n",
    "    n_neighbors = trail.suggest_int(\"n_neighbors\", 5, 20)\n",
    "\n",
    "    ## Build Model\n",
    "    model = KNeighborsClassifier( n_jobs=-1, \n",
    "                                  n_neighbors = n_neighbors\n",
    "                                 )\n",
    "\n",
    "    ## Set TS-CV\n",
    "    tscv = TimeSeriesSplit(n_splits=TimeSplit, gap=1)\n",
    "\n",
    "    ## Create Variable\n",
    "    fscore = []\n",
    "\n",
    "    ## Run for the different training and testing data from TS-CV splits \n",
    "    for idx in tscv.split(x):\n",
    "        train_idx, test_idx = idx[0], idx[1]\n",
    "\n",
    "        xtrain = x[train_idx]\n",
    "        ytrain = y[train_idx]\n",
    "\n",
    "        xtest = x[test_idx]\n",
    "        ytest = y[test_idx]\n",
    "\n",
    "        model.fit(xtrain, ytrain)\n",
    "        preds = model.predict(xtest)\n",
    "\n",
    "        cv_f1 = f1_score(ytest, preds, average='weighted')\n",
    "        fscore.append(cv_f1)\n",
    "\n",
    "    return np.mean(fscore)\n",
    "\n",
    "\n",
    "\n",
    "#______________________________________________________________________________\n",
    "\n",
    "\n",
    "###  3) SVC\n",
    "\n",
    "def optimize_SVC(trail, x, y):\n",
    "\n",
    "    ## Specify Hyperparameters range\n",
    "    kernel = trail.suggest_categorical(\"kernel\", ['linear', 'rbf'])\n",
    "    \n",
    "    C = trail.suggest_float(\"C\", 0.1, 10)\n",
    "\n",
    "    ## Build Model\n",
    "    model = SVC( probability=True, # Allow probability estimate\n",
    "                 class_weight = cwts(y), # Balance Class\n",
    "                 kernel= kernel, \n",
    "                 C=C, # Regularization parameter. A larger value of C implies less regularization.\n",
    "                )\n",
    "\n",
    "\n",
    "    ## Set TS-CV\n",
    "    tscv = TimeSeriesSplit(n_splits=TimeSplit, gap=1)\n",
    "\n",
    "    ## Create Variable\n",
    "    fscore = []\n",
    "\n",
    "    ## Run for the different training and testing data from TS-CV splits \n",
    "    for idx in tscv.split(x):\n",
    "        train_idx, test_idx = idx[0], idx[1]\n",
    "\n",
    "        xtrain = x[train_idx]\n",
    "        ytrain = y[train_idx]\n",
    "\n",
    "        xtest = x[test_idx]\n",
    "        ytest = y[test_idx]\n",
    "\n",
    "        model.fit(xtrain, ytrain)\n",
    "        preds = model.predict(xtest)\n",
    "\n",
    "        cv_f1 = f1_score(ytest, preds, average='weighted')\n",
    "        fscore.append(cv_f1)\n",
    "\n",
    "    return np.mean(fscore)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Tuning Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    ## New function by \"partially\" fixing some hiperparameters from optimization\n",
    "    optimization_function = partial(optimize_RF, x=X, y=y)\n",
    "\n",
    "    ## Create a Max the Objective Function\n",
    "    rf_study = optuna.create_study(study_name='rf_optuna', \n",
    "                                   direction=\"maximize\")\n",
    "\n",
    "    ## Perform n optimization trials using the Optimization function\n",
    "    rf_study.optimize(optimization_function, n_trials=n_trials)\n",
    "\n",
    "    ## Save Best Parameters\n",
    "    rf_opt_params = pd.DataFrame({'n_estimators':  [rf_study.best_params['n_estimators'] ],\n",
    "                                  'max_depth': [rf_study.best_params['max_depth'] ],\n",
    "                                  'max_features':  [rf_study.best_params['max_features'] ]\n",
    "                                 })\n",
    "    \n",
    "    ## Save to CSV\n",
    "    rf_opt_params.to_csv('../Output/rf_hyperparam_tun.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot Hyperparameteres\n",
    "plot_slice(rf_study).write_image(\"../../Plot/Tuning/hyp_set_rf.pdf\")\n",
    "plot_slice(rf_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## Plot - Optimization History\n",
    "plot_optimization_history(rf_study).write_image(\"../../Plot/Tuning/opt_hist_rf.pdf\")\n",
    "plot_optimization_history(rf_study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Tuning KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    ## New function by \"partially\" fixing some hiperparameters from optimization\n",
    "    optimization_function = partial(optimize_KNN, x=X, y=y)\n",
    "\n",
    "    ## Create a Max the Objective Function\n",
    "    knn_study = optuna.create_study(study_name='knn_optuna', \n",
    "                                    direction=\"maximize\")\n",
    "\n",
    "    ## Perform n optimization trials using the Optimization function\n",
    "    knn_study.optimize(optimization_function, n_trials=n_trials)\n",
    "\n",
    "    ## Save Best Parameters\n",
    "    knn_opt_params = pd.DataFrame({'n_neighbors':  [knn_study.best_params['n_neighbors'] ]   })\n",
    "    \n",
    "    ## Save to CSV\n",
    "    knn_opt_params.to_csv('../Output/knn_hyperparam_tun.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot Hyperparameteres\n",
    "plot_slice(knn_study).write_image(\"../../Plot/Tuning/hyp_set_knn.pdf\")\n",
    "plot_slice(knn_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## Plot - Optimization History\n",
    "plot_optimization_history(knn_study).write_image(\"../../Plot/Tuning/opt_hist_knn.pdf\")\n",
    "plot_optimization_history(knn_study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Tuning SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    ## New function by \"partially\" fixing some hiperparameters from optimization\n",
    "    optimization_function = partial(optimize_SVC, x=X, y=y)\n",
    "\n",
    "    ## Create a Max the Objective Function\n",
    "    svc_study = optuna.create_study(study_name='svc_optuna', \n",
    "                                    direction=\"maximize\")\n",
    "\n",
    "    ## Perform n optimization trials using the Optimization function\n",
    "    svc_study.optimize(optimization_function, n_trials=n_trials)\n",
    "\n",
    "    ## Save Best Parameters\n",
    "    svc_opt_params = pd.DataFrame({'kernel':  [svc_study.best_params['kernel'] ],\n",
    "                                   'C': [svc_study.best_params['C'] ],\n",
    "                                 })\n",
    "    \n",
    "    ## Save to CSV\n",
    "    svc_opt_params.to_csv('../Output/svc_hyperparam_tun.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot Hyperparameteres\n",
    "plot_slice(svc_study).write_image(\"../../Plot/Tuning/hyp_set_svc.pdf\")\n",
    "plot_slice(svc_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## Plot - Optimization History\n",
    "plot_optimization_history(svc_study).write_image(\"../../Plot/Tuning/opt_hist_svc.pdf\")\n",
    "plot_optimization_history(svc_study)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "188775c9ea53fc35b016103c82c46ac82e09a5f3430e273dbc1240ad0d8f2a3d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
