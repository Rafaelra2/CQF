{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mod' Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the processed data and the mod architecture, I compute the predictions and the Tensor Board for:\n",
    "\n",
    "- LSTM: 1 Hidden Layer no Tunning\n",
    "- LSTM: 2 Hidden Layers no Tunning\n",
    "- LSTM: 3 Hidden Layers no Tunning\n",
    "\n",
    "#\n",
    "\n",
    "- LSTM: 1 Hidden Layer with Tunning\n",
    "- LSTM: 2 Hidden Layers with Tunning\n",
    "- LSTM: 3 Hidden Layers with Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Requirements\n",
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import Basic\n",
    "import os\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "import pydot\n",
    "\n",
    "# Statistics\n",
    "import random # functions for generating random numbers\n",
    "\n",
    "# Data Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_ta as ta # Extends the capabilities of Pandas for financial technical analysis\n",
    "\n",
    "\n",
    "# Plotting & Outputs\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "# Import custom transformer\n",
    "# from helper import DayTransformer, TimeTransformer\n",
    "\n",
    "## Feature Engineering\n",
    "\n",
    "# Feature Selection\n",
    "from boruta import BorutaPy\n",
    "\n",
    "# Feature Transformation\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "\n",
    "# Organize Training and Testing Data\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "\n",
    "## Build and Evaluate Model\n",
    "\n",
    "# Tensorflow - LSTM Models \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator \n",
    "\n",
    "# Model Architecture and Optimization\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import LSTM, BatchNormalization\n",
    "from tensorflow.keras.losses import BinaryCrossentropy \n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.layers import Dropout, Dense, Flatten\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop \n",
    "\n",
    "# Ouput Metrics\n",
    "from tensorflow.keras.metrics import BinaryAccuracy, Accuracy, AUC, Precision, Recall\n",
    "\n",
    "# Model Evaluation\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import RocCurveDisplay, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Tuning - Kerastuner\n",
    "import keras_tuner as kt\n",
    "from kerastuner import HyperParameter, HyperParameters\n",
    "from kerastuner.tuners import RandomSearch, Hyperband, BayesianOptimization\n",
    "\n",
    "pd.set_option('display.max_columns', 30)\n",
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Other functions and Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Other Functions\n",
    "from aux_functions import *\n",
    "\n",
    "# Import Model Architecture\n",
    "from mod_architecture import *\n",
    "\n",
    "# Import Tune Models\n",
    "from mod_tuning import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Results Path\n",
    "m_fit_path = Path('../Output/Model_fit')\n",
    "m_arch_path = Path('../Plot/Model_architecture')\n",
    "tb_path = Path('../Output/TensorBoard')\n",
    "tun_path = Path('../Output/Tuning')\n",
    "text_path = Path('../Text')\n",
    "\n",
    "if not m_fit_path.exists():\n",
    "    m_fit_path.mkdir(parents=True)\n",
    "    \n",
    "if not m_arch_path.exists():\n",
    "    m_arch_path.mkdir(parents=True)\n",
    "    \n",
    "if not tb_path.exists():\n",
    "    tb_path.mkdir(parents=True)\n",
    "    \n",
    "if not tun_path.exists():\n",
    "    tun_path.mkdir(parents=True)\n",
    "    \n",
    "if not text_path.exists():\n",
    "    text_path.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random\n",
    "random = 10\n",
    "set_seeds(seed=random)\n",
    "\n",
    "## Load dataset\n",
    "df = pd.read_csv('../../Input/feat_final.csv', index_col=0)\n",
    "\n",
    "## Set Features and Target Variable\n",
    "X, y = df.iloc[:,:-1].values, df.iloc[:,-1].values\n",
    "\n",
    "## Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random, shuffle=False)  \n",
    "\n",
    "## Output the train and test data size\n",
    "print(f\"Train and Test Size {X_train.shape}, {y_train.shape}, {X_test.shape}, {y_test.shape}\")\n",
    "\n",
    "## Sequence length\n",
    "seqlen = pd.read_csv('../Output/Variables/seqlen.csv', header=None).iloc[0, 0]\n",
    "\n",
    "## Number of features\n",
    "numfeat = X.shape[1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate train and test sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate train and test sequence data\n",
    "batch_size = 64\n",
    "tsg_train  = TimeseriesGenerator(X_train, y_train, length=seqlen, batch_size=batch_size)\n",
    "tsg_test   = TimeseriesGenerator(X_test, y_test, length=seqlen, batch_size=batch_size)\n",
    "\n",
    "## Verify Sequence\n",
    "# for i in range(len(tsg_train)): # len(tsg_train) is the number of sequences\n",
    "#     a, b = tsg_train[i]\n",
    "#     print(a.shape, b.shape) # Batch size in each sequence, length, feature size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - LSTM: 1 Hidden Layer no Tunning \n",
    "### Fit/Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Intent Model \n",
    "lstm_11 = create_lstm_h1_noTun( hu=10, lookback=seqlen, features=numfeat )\n",
    "\n",
    "## Plot Model Architecture\n",
    "# plot_model(lstm_11, \n",
    "#           to_file= (m_arch_path / 'model11.pdf').as_posix(),\n",
    "#           show_shapes=True, # Include the shapes of the data (input and output shapes of the layers)\n",
    "#           show_layer_names=True) # Displayed the names of the layers in the diagram\n",
    "\n",
    "# ## Summary\n",
    "# lstm_11.summary()\n",
    "\n",
    "## Model fitting -  Since I have a balanced data, no need: class_weight\n",
    "lstm_11.fit( tsg_train, # Using generate TS data\n",
    "             epochs= 400, # 500 \n",
    "             verbose=1, \n",
    "             shuffle=False,\n",
    "             callbacks= create_callbacks(\n",
    "                        filepath = (m_fit_path / 'm11.h5').as_posix(),\n",
    "                        log_dir=os.path.join(\"../Output/TensorBoard/Fiting/m11_logs\", dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")), \n",
    "                        monitor='loss', \n",
    "                        patience=10 )                              \n",
    "            )\n",
    "\n",
    "## Load Model\n",
    "#model = load_model( (m_fit_path / 'm11.h5').as_posix() )\n",
    "\n",
    "### Export Predictions\n",
    "\n",
    "## Predic Model - Train\n",
    "ypred_train = np.where( lstm_11.predict(tsg_train, verbose=False) > 0.5, 1, 0)\n",
    "\n",
    "## Predic Model - Test\n",
    "ypred_test  = np.where( lstm_11.predict(tsg_test, verbose=False) > 0.5, 1, 0)\n",
    "\n",
    "## Probability Model\n",
    "yprob = lstm_11.predict(tsg_test)\n",
    "\n",
    "## Save\n",
    "pd.DataFrame(ypred_train).to_csv('../Output/Variables/m11_ypred_train.csv', index=False)\n",
    "pd.DataFrame(ypred_test).to_csv('../Output/Variables/m11_ypred_test.csv', index=False)\n",
    "pd.DataFrame(yprob).to_csv('../Output/Variables/m11_yprob.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - LSTM: 2 Hidden Layer no Tunning \n",
    "### Fit/Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Intent Model \n",
    "lstm_21 = create_lstm_h2_noTun( hu=10, dropout=0.5, lookback=seqlen, features=numfeat )\n",
    "\n",
    "## Plot Model Architecture\n",
    "# plot_model(lstm_21, \n",
    "#           to_file= (m_arch_path / 'model21.pdf').as_posix(),\n",
    "#           show_shapes=True, # Include the shapes of the data (input and output shapes of the layers)\n",
    "#           show_layer_names=True) # Displayed the names of the layers in the diagram\n",
    "\n",
    "# # Summary\n",
    "# lstm_21.summary()\n",
    "\n",
    "## Model fitting - Since I have a balanced data, no need: class_weight\n",
    "lstm_21.fit( tsg_train, # Using generate TS data\n",
    "             epochs= 500, # 1000\n",
    "             verbose=1, \n",
    "             shuffle=False,\n",
    "             callbacks= create_callbacks(\n",
    "                        filepath = (m_fit_path / 'm21.h5').as_posix(),\n",
    "                        log_dir=os.path.join(\"../Output/TensorBoard/Fiting/m21_logs\", dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")), \n",
    "                        monitor='loss', \n",
    "                        patience=10 )                              \n",
    "            )\n",
    "\n",
    "## Load Model\n",
    "#model = load_model( (m_fit_path / 'm21.h5').as_posix() )\n",
    "\n",
    "### Export Predictions\n",
    "\n",
    "## Predic Model - Train\n",
    "ypred_train = np.where( lstm_21.predict(tsg_train, verbose=False) > 0.5, 1, 0)\n",
    "\n",
    "## Predic Model - Test\n",
    "ypred_test  = np.where( lstm_21.predict(tsg_test, verbose=False) > 0.5, 1, 0)\n",
    "\n",
    "## Probability Model\n",
    "yprob = lstm_21.predict(tsg_test)\n",
    "\n",
    "## Save\n",
    "pd.DataFrame(ypred_train).to_csv('../Output/Variables/m21_ypred_train.csv', index=False)\n",
    "pd.DataFrame(ypred_test).to_csv('../Output/Variables/m21_ypred_test.csv', index=False)\n",
    "pd.DataFrame(yprob).to_csv('../Output/Variables/m21_yprob.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 - LSTM: 3 Hidden Layer no Tunning \n",
    "### Fit/Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Intent Model \n",
    "lstm_31 = create_lstm_h3_noTun( hu=10, dropout=0.5, lookback=seqlen, features=numfeat )\n",
    "\n",
    "## Plot Model Architecture\n",
    "# plot_model(lstm_31, \n",
    "#           to_file= (m_arch_path / 'model31.pdf').as_posix(),\n",
    "#           show_shapes=True, # Include the shapes of the data (input and output shapes of the layers)\n",
    "#           show_layer_names=True) # Displayed the names of the layers in the diagram\n",
    "\n",
    "# # Summary\n",
    "# lstm_31.summary()\n",
    "\n",
    "## Model fitting - Since I have a balanced data, no need: class_weight\n",
    "lstm_31.fit( tsg_train, # Using generate TS data\n",
    "             epochs= 300,  \n",
    "             verbose=1, \n",
    "             shuffle=False,\n",
    "             callbacks= create_callbacks(\n",
    "                        filepath = (m_fit_path / 'm31.h5').as_posix(),\n",
    "                        log_dir=os.path.join(\"../Output/TensorBoard/Fiting/m31_logs\", dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")), \n",
    "                        monitor='loss', \n",
    "                        patience=10 )                              \n",
    "            )\n",
    "\n",
    "## Load Model\n",
    "#model = load_model( (m_fit_path / 'm31.h5').as_posix() )\n",
    "\n",
    "### Export Predictions\n",
    "\n",
    "## Predic Model - Train\n",
    "ypred_train = np.where( lstm_31.predict(tsg_train, verbose=False) > 0.5, 1, 0)\n",
    "\n",
    "## Predic Model - Test\n",
    "ypred_test  = np.where( lstm_31.predict(tsg_test, verbose=False) > 0.5, 1, 0)\n",
    "\n",
    "## Probability Model\n",
    "yprob = lstm_31.predict(tsg_test)\n",
    "\n",
    "## Save\n",
    "pd.DataFrame(ypred_train).to_csv('../Output/Variables/m31_ypred_train.csv', index=False)\n",
    "pd.DataFrame(ypred_test).to_csv('../Output/Variables/m31_ypred_test.csv', index=False)\n",
    "pd.DataFrame(yprob).to_csv('../Output/Variables/m31_yprob.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - LSTM: 1 Hidden Layer with Tunning \n",
    "### Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit Hyperband Algorithm\n",
    "hbtuner1 = kt.Hyperband( tune_lstm_h1, # Model\n",
    "                         objective=\"val_loss\",\n",
    "                         max_epochs=20, #  20\n",
    "                         hyperband_iterations=2, #3\n",
    "                         directory=\"../Output/Tuning\",\n",
    "                         project_name=\"hb_m1\",\n",
    "                         overwrite=False,\n",
    "                         seed=10\n",
    "                       )  \n",
    "\n",
    "## Run Hyperband Algorithm - Since I have a balanced data, no need: class_weight\n",
    "hbtuner1.search( tsg_train, \n",
    "                 epochs=15, #25 more restrictive\n",
    "                 validation_data=tsg_test, \n",
    "                 callbacks= tune_callbacks( monitor='loss', patience=3), # Set patience\n",
    "                 shuffle=False,\n",
    "                 verbose=0  #  disable progress bars\n",
    "                )\n",
    "\n",
    "## Display tuning results \n",
    "#hbtuner1.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit/Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Best Parameters\n",
    "best_hb1 = hbtuner1.get_best_hyperparameters()[0]\n",
    "\n",
    "## Intent Model \n",
    "lstm_12 = create_lstm_h1_Tun( best_hb1, lookback=seqlen, features=numfeat  )\n",
    "\n",
    "## Print\n",
    "print(\"Best Hyperparameters:\")\n",
    "for hp in best_hb1.space:\n",
    "    print(f\"{hp.name}: {best_hb1.get(hp.name)}\")\n",
    "\n",
    "# ## Plot Model Architecture\n",
    "# plot_model(lstm_12, \n",
    "#           to_file= (m_arch_path / 'model12.pdf').as_posix(),\n",
    "#           show_shapes=True, # Include the shapes of the data (input and output shapes of the layers)\n",
    "#           show_layer_names=True) # Displayed the names of the layers in the diagram\n",
    "\n",
    "# ## Summary\n",
    "# lstm_12.summary()\n",
    "\n",
    "## Model fitting - Since I have a balanced data, no need: class_weight\n",
    "lstm_12.fit( tsg_train, # Using generate TS data\n",
    "             epochs= 200, \n",
    "             verbose=1, \n",
    "             shuffle=False,\n",
    "             callbacks= create_callbacks(\n",
    "                        filepath = (m_fit_path / 'm12.h5').as_posix(),\n",
    "                        log_dir=os.path.join(\"../Output/TensorBoard/Fiting/m12_logs\", dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")), \n",
    "                        monitor='loss', \n",
    "                        patience=5 )                              \n",
    "            )\n",
    "\n",
    "## Load Model\n",
    "#model = load_model( (m_fit_path / 'm12.h5').as_posix() )\n",
    "\n",
    "### Export Predictions\n",
    "\n",
    "## Predic Model - Train\n",
    "ypred_train = np.where( lstm_12.predict(tsg_train, verbose=False) > 0.5, 1, 0)\n",
    "\n",
    "## Predic Model - Test\n",
    "ypred_test  = np.where( lstm_12.predict(tsg_test, verbose=False) > 0.5, 1, 0)\n",
    "\n",
    "## Probability Model\n",
    "yprob = lstm_12.predict(tsg_test)\n",
    "\n",
    "## Save\n",
    "pd.DataFrame(ypred_train).to_csv('../Output/Variables/m12_ypred_train.csv', index=False)\n",
    "pd.DataFrame(ypred_test).to_csv('../Output/Variables/m12_ypred_test.csv', index=False)\n",
    "pd.DataFrame(yprob).to_csv('../Output/Variables/m12_yprob.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - LSTM: 2 Hidden Layer with Tunning \n",
    "### Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit Hyperband Algorithm\n",
    "hbtuner2 = kt.Hyperband( tune_lstm_h2, # Model\n",
    "                         objective=\"val_loss\",\n",
    "                         max_epochs=20, #  20\n",
    "                         hyperband_iterations=2, #3\n",
    "                         directory=\"../Output/Tuning\",\n",
    "                         project_name=\"hb_m2\",\n",
    "                         overwrite=False,\n",
    "                         seed=10\n",
    "                       )\n",
    "\n",
    "## Run Hyperband Algorithm - Since I have a balanced data, no need: class_weight\n",
    "hbtuner2.search( tsg_train, \n",
    "                 epochs=15, #25  # more restrictive\n",
    "                 validation_data=tsg_test, \n",
    "                 shuffle=False,\n",
    "                 callbacks= tune_callbacks( monitor='loss', patience=3), # Set patience\n",
    "                 verbose=0  #  disable progress bars\n",
    "                )\n",
    "\n",
    "\n",
    "## Display tuning results \n",
    "#hbtuner2.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit/Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Best Parameters\n",
    "best_hb2 = hbtuner2.get_best_hyperparameters()[0]\n",
    "\n",
    "## Intent Model \n",
    "lstm_22 = create_lstm_h2_Tun( best_hb2, lookback=seqlen, features=numfeat  )\n",
    "\n",
    "## Print\n",
    "#print(\"Best Hyperparameters:\")\n",
    "# for hp in best_hb2.space:\n",
    "#     print(f\"{hp.name}: {best_hb2.get(hp.name)}\")\n",
    "\n",
    "\n",
    "# ## Plot Model Architecture\n",
    "# plot_model(lstm_22, \n",
    "#           to_file= (m_arch_path / 'model22.pdf').as_posix(),\n",
    "#           show_shapes=True, # Include the shapes of the data (input and output shapes of the layers)\n",
    "#           show_layer_names=True) # Displayed the names of the layers in the diagram\n",
    "\n",
    "# # Summary\n",
    "# lstm_22.summary()\n",
    "\n",
    "## Model fitting - Since I have a balanced data, no need: class_weight\n",
    "lstm_22.fit( tsg_train, # Using generate TS data\n",
    "             epochs= 200, # 500 1000\n",
    "             verbose=1, \n",
    "             shuffle=False,\n",
    "             callbacks= create_callbacks(\n",
    "                        filepath = (m_fit_path / 'm22.h5').as_posix(),\n",
    "                        log_dir=os.path.join(\"../Output/TensorBoard/Fiting/m22_logs\", dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")), \n",
    "                        monitor='loss', \n",
    "                        patience=5 )                              \n",
    "            )\n",
    "\n",
    "## Load Model\n",
    "#model = load_model( (m_fit_path / 'm22.h5').as_posix() )\n",
    "\n",
    "### Export Predictions\n",
    "\n",
    "## Predic Model - Train\n",
    "ypred_train = np.where( lstm_22.predict(tsg_train, verbose=False) > 0.5, 1, 0)\n",
    "\n",
    "## Predic Model - Test\n",
    "ypred_test  = np.where( lstm_22.predict(tsg_test, verbose=False) > 0.5, 1, 0)\n",
    "\n",
    "## Probability Model\n",
    "yprob = lstm_22.predict(tsg_test)\n",
    "\n",
    "## Save\n",
    "pd.DataFrame(ypred_train).to_csv('../Output/Variables/m22_ypred_train.csv', index=False)\n",
    "pd.DataFrame(ypred_test).to_csv('../Output/Variables/m22_ypred_test.csv', index=False)\n",
    "pd.DataFrame(yprob).to_csv('../Output/Variables/m22_yprob.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - LSTM: 3 Hidden Layer with Tunning \n",
    "### Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit Hyperband Algorithm\n",
    "hbtuner3 = kt.Hyperband( tune_lstm_h3, # Model\n",
    "                         objective=\"val_loss\",\n",
    "                         max_epochs=20, # 5\n",
    "                         hyperband_iterations=2, #15\n",
    "                         directory=\"../Output/Tuning\",\n",
    "                         project_name=\"hb_m3\",\n",
    "                         overwrite=False,\n",
    "                         seed=10\n",
    "                       )\n",
    "\n",
    "## Run Hyperband Algorithm - Since I have a balanced data, no need: class_weight\n",
    "hbtuner3.search( tsg_train, \n",
    "                 epochs=15, #50  # more restrictive\n",
    "                 validation_data=tsg_test, \n",
    "                 callbacks= tune_callbacks( monitor='loss', patience=5), # Set patience\n",
    "                 shuffle=False,\n",
    "                 verbose=0  #  disable progress bars\n",
    "                )\n",
    "\n",
    "## Display tuning results \n",
    "#hbtuner3.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Best Parameters\n",
    "best_hb3 = hbtuner3.get_best_hyperparameters()[0]\n",
    "\n",
    "## Intent Model \n",
    "lstm_32 = create_lstm_h3_Tun( best_hb3, lookback=seqlen, features=numfeat  )\n",
    "\n",
    "## Print\n",
    "#print(\"Best Hyperparameters:\")\n",
    "# for hp in best_hb3.space:\n",
    "#     print(f\"{hp.name}: {best_hb3.get(hp.name)}\")\n",
    "\n",
    "\n",
    "# ## Plot Model Architecture\n",
    "# plot_model(lstm_32, \n",
    "#           to_file= (m_arch_path / 'model32.pdf').as_posix(),\n",
    "#           show_shapes=True, # Include the shapes of the data (input and output shapes of the layers)\n",
    "#           show_layer_names=True) # Displayed the names of the layers in the diagram\n",
    "\n",
    "# # Summary\n",
    "# lstm_32.summary()\n",
    "\n",
    "## Model fitting - Since I have a balanced data, no need: class_weight\n",
    "lstm_32.fit( tsg_train, # Using generate TS data\n",
    "             epochs= 200, # 500 \n",
    "             verbose=1, \n",
    "             shuffle=False,\n",
    "             callbacks= create_callbacks(\n",
    "                        filepath = (m_fit_path / 'm32.h5').as_posix(),\n",
    "                        log_dir=os.path.join(\"../Output/TensorBoard/Fiting/m32_logs\", dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")), \n",
    "                        monitor='loss', \n",
    "                        patience=5 )                              \n",
    "            )\n",
    "\n",
    "## Load Model\n",
    "#model = load_model( (m_fit_path / 'm32.h5').as_posix() )\n",
    "\n",
    "### Export Predictions\n",
    "\n",
    "## Predic Model - Train\n",
    "ypred_train = np.where( lstm_32.predict(tsg_train, verbose=False) > 0.5, 1, 0)\n",
    "\n",
    "## Predic Model - Test\n",
    "ypred_test  = np.where( lstm_32.predict(tsg_test, verbose=False) > 0.5, 1, 0)\n",
    "\n",
    "## Probability Model\n",
    "yprob = lstm_32.predict(tsg_test)\n",
    "\n",
    "## Save\n",
    "pd.DataFrame(ypred_train).to_csv('../Output/Variables/m32_ypred_train.csv', index=False)\n",
    "pd.DataFrame(ypred_test).to_csv('../Output/Variables/m32_ypred_test.csv', index=False)\n",
    "pd.DataFrame(yprob).to_csv('../Output/Variables/m32_yprob.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard \n",
    "# Access: http://localhost:6006\n",
    "# %reload_ext tensorboard\n",
    "# Clear: \n",
    "# cmd prompt --> taskkill /IM \"tensorboard.exe\" /F\n",
    "# AppData/Local/Temp/.tensorboard-info\n",
    "\n",
    "%tensorboard --logdir ../Output/TensorBoard/Fiting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "188775c9ea53fc35b016103c82c46ac82e09a5f3430e273dbc1240ad0d8f2a3d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
